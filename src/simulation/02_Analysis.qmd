---
title: "Supplementary materials to the paper “Machine learning prediction models for donor Hb deferral: can we use the same models across countries?”"
author: Amber Meulenbeld, Jarkko Toivonen, Tinus Brits, Ronel Swanevelder, Dorien de Clippel, Veerle Compernolle, Surendra Karki, Marijke Welvaert, Katja van den Hurk, Joost van Rosmalen, Emmanuel Lesaffre, Mart Janssen and Mikko Arvas
bibliography: bibliography.bib
format:
  html:
    code-fold: true
#    page-layout: full
    toc: true
    embed-resources: true
---

# Initialization

```{r warning=F, message=F}
#| label: packages
#| code-fold: false

library(stats)
library("randomForest")
library(ROCR)
library(kableExtra)

source("01_Functions.R")
```

```{r setup, include=FALSE}
# Register an inline hook
knitr::knit_hooks$set(inline = function(x) {
  paste(custom_print(x), collapse = ", ")
})
# Define a function factory (from @eipi10 answer)
op <- function(d = 2) {
  function(x) sprintf(paste0("%1.", d, "f"), x)
}
# Use a closure
custom_print <- op()

```

# Parameters

```{r}
#| label: parameters
#| code-summary: "Parameters"

n<-1e4          # nr of simulation records
PopMean<-9.4    # population mean Hb
PopSd  <-0.7    # standard deviation of population Hb
MeasSd <-0.5    # standard deviation of Hb measurement
threshold<-8.4  # deferral threshold
nr_meas<-2      # number of measurements performed
propdef<-.05    # proportion deferrals to be analysed

# proportion deferrals to be analysed
props<-c(.01, .02, .04, .08, .16)
# measurement variability to be analysed
MeasSds<-c(0.5, 0.25)

# set train/test/validation proportions
trainprop<-0.80 # proportion training data
testprop <-0.20 # proportion test data

# set number of simulations
nrsimulations <- 300

set.seed(1)
```

To evaluate the behaviour of prediction models for donor deferral we simulated an artificial donor population with mean Hb level of `r PopMean` and a Sd of `r PopSd` mmol/L. We also presumed that the Hb level of donors would be measured with a measurement error with a standard deviation of `r MeasSd` mmol/L (without any bias).

# Data simulation

```{r}
#| label: create dataset
#| results: false
#| code-summary: "Code data simulation"

set.seed(1)

(d1<-createDatasets(propdef=0.03, printresults = F))

```

```{r}
#| label: fig-distr
#| fig-cap: Population Hb distribution and measured Hb distribution for different measurement levels of measurement error (Sd of 0.50 and 0.25 mmol/L respectively).
#| code-summary: "Code plot"
#| cap-location: margin
set.seed(1)

  plot(density(data_train$true), lwd=2, main="", xlab="Hb level [mmol/L]")
  abline(v=threshold,col=8)
  dens<-density(data_train$meas1, bw=.13)
  lines(dens$x,dens$y, col="red", lwd=2)
  dens<-density(data_test$meas1, bw=.17)
  lines(dens$x,dens$y, col="green", lwd=2)
  MeasSd<-MeasSds[2]
  createNewTestDataset()
  defperc1 <- sum(data_train$deferral==1)/nrow(data_train)
  defperc2 <- sum(data_test$deferral==1)/nrow(data_test)
  defperc3 <- sum(data_test$meas1<threshold)/nrow(data_test)
  MeasSd<-MeasSds[1]
  legend("topright", c("Population distribution","Measurement Sd=0.50","Measurement Sd=0.25"), 
         lty=c(1,1,1), col=c(1,2,3), lwd=c(2,2,2))

```

This population has a deferral rate of `r d1[1]*100`%. This will lead to an observed proportion of `r defperc1*100`% of measurements below the deferral threshold (see @fig-distr). In case the measurement variation would be only half as big (0.25 mmol/L) there would still be a deferral rate of `r defperc2*100`%.

# Model fitting

```{r no-decimals}
#| label: no-decimals
#| include: false
custom_print <- op(d = 0)
```

We simulated a dataset of `r nrow(data_test)+nrow(data_train)` donations from the above population of which the Hb levels were measured with a standard deviation of `r MeasSd`  mmol/L where we did two subsequent measurements. A random forest model was trained on `r nrow(data_train)/(nrow(data_test)+nrow(data_train))*100`% of the data to predict donor deferral on the second Hb measurement using the Hb levels from the first measurement as a predictor. The quality of the deferral prediction model was expressed as the Corrected Area Under the Precision Recall curve (cAUPR) and as the area under the Receiver Operating Characteristic curve (AUROC) which were derived from applying the model to a test data set consisting of the remaining `r nrow(data_test)/(nrow(data_test)+nrow(data_train))*100`% of the data. The correction applied for the AUPR estimates consists of a subtraction of the overall deferral rate, as reported earlier (@vinkenoog_international_2023).

@fig-AUPRcurve shows the (uncorrected) precision recall curve and @fig-AUROCurve the ROC curve for a training and test dataset. 

```{r}
#| label: fit model and calculate statistics
#| echo: true
#| results: false
#| code-summary: "Code RF model"
set.seed(1)

# Now fit a model
bestmtry <- tuneRF(data_train,data_train$deferral,stepFactor=1.20, improve = 0.010, trace=T, plot= F) 
RFmodel <- randomForest(deferral~meas1,data=data_train)
RFmodel
```



```{r}
#| label: performance 
#| code-summary: "Code performance statistics"
set.seed(1)

# Calculate various prediction characteristics 
pred_train_p = predict(RFmodel, type = "prob")
perf_train_data = prediction(pred_train_p[,2], data_train$deferral)
train_perf = performance(perf_train_data, "prec","sens")
train_perf2 = performance(perf_train_data, "sens","spec")
train_perf2a = performance(perf_train_data, "tpr","fpr")
train_aucpr = performance(perf_train_data, "aucpr")
train_auc = performance(perf_train_data, "auc")

pred_test_p = predict(RFmodel, newdata = data_test, type= "prob")
perf_test_data = prediction(pred_test_p[,2], data_test$deferral)
test_perf = performance(perf_test_data, "prec","sens")
test_perf2 = performance(perf_test_data, "sens","spec")
test_perf2a = performance(perf_test_data, "tpr","fpr")
test_aucpr = performance(perf_test_data, "aucpr")
test_auc = performance(perf_test_data, "auc")
```


```{r}
#| label: fig-AUPRcurve
#| fig-cap-location: margin
#| fig-cap: Precision-recall curve of a Random Forrest deferral prediction model when applied to training and test data sets with a 3% deferral rate.
#| code-summary: "Code plot"
set.seed(1)

plot(train_perf, main="Precision-recall curve for Random Forest",col=2,lwd=2, xlab="Recall")
lines(unlist(train_perf@x.values),unlist(train_perf@y.values), col=2, lwd=2)
lines(unlist(test_perf@x.values),unlist(test_perf@y.values), col=3, lwd=2)
legend("topright", c(paste0("Training data (AUPR=",round(train_aucpr@y.values[[1]],3),")"), paste0("Test data (AUPR=",round(test_aucpr@y.values[[1]],3),")")), col=c(2,3), lwd=c(2,2))
```

```{r}
#| label: fig-AUROCurve
#| fig-cap-location: margin
#| fig-cap: ROC of a Random Forrest deferral prediction model when applied to training and test data sets with a 3% deferral rate.
#| code-summary: "Code plot"
set.seed(1)

plot(train_perf2a, main="ROC-Curve for Random Forest",col=2,lwd=2)
lines(unlist(train_perf2a@x.values),unlist(train_perf2a@y.values), col=2, lwd=2)
lines(unlist(test_perf2a@x.values),unlist(test_perf2a@y.values), col=3, lwd=2)
legend("bottomright", c(paste0("Training data (AUC=",round(train_auc@y.values[[1]],3),")"), paste0("Test data (AUC=",round(test_auc@y.values[[1]],3),")")), col=c(2,3), lwd=c(2,2))
```

# Results

We repeated the simulation procedure `r nrsimulations` times for various deferral rates (`r props*100`%) as shown in @fig-defrates. Deferral rates were set by adjusting the mean population Hb levels accordingly. @fig-AUPR and @fig-AUROC show the cAUPR and AUROC estimates for RF models trained and tested at various deferral rates. These figures clearly show that both the cAUPR and AUROC estimates increase with an increase in donor deferral rate. Also, these figures show that the cAUPR is more sensitive to changes between the models tested at various deferral rates. This is because the AUPR is sensitive changes in the positive predictive value (or precision) of the model whereas the AUROC is not. This is the main reason that AUPR performance measures are recommended for evaluating models with unbalanced binary class outcomes (@saito_precision-recall_2015). In @fig-AUPRexchange the AUPR estimates for RF models for various deferral rates are shown again, but now the results for models trained at all deferral rate levels applied to all deferral rate test data are shown as well. These results are provided in numerical form in @tbl-cAUPR. @fig-AUROCexchange and @tbl-AUROC provide the same information for AUROC estimates. From @fig-AUPRexchange and @fig-AUROCexchange it can be found that models trained on data sets with higher deferral rates generally have a (slightly) better performance than model trained on dataset with lower deferral rates. @fig-AUPRbothmeaserr and @fig-AUROCbothmeaserr show the result of applying these models to test datasets with a lower measurement error (0.25 instead of 0.50 mmol/L) but identical preset deferral rates. These figures clearly show the increase in performance of the prediction models when measurements are more accurate. The data presented in these figures is also provided in @tbl-cAUPR and @tbl-AUROC.

```{r}
#| label: analyse training and test data
#| include: false

createDatasets(propdef=.03)
analyseTrainingData()
analyseTestData()
createNewTestDataset()
analyseTestData()
```

```{r}
#| label: analyse population means
#| include: false

PopMeans<-matrix(NA, nrow=length(props), ncol=length(MeasSds))
#for all Sd levels considered
for (j in 1:length(MeasSds)) {
  print(paste("Setpoints for a measurement Sd of", round(MeasSds[j],2)))
  for (i in 1:length(props)){
    fo<-function(x) (pnorm(threshold, mean=x, sqrt(PopSd^2+MeasSds[j]^2))-props[i])^2
    sol<-optimize(fo, c(0,2*PopMean))
    PopMeans[i,j] <- sol$minimum
    # print  population mean, probability of population true level of deferral, 
    # and probability of measured deferrals
    print(paste(sol$minimum, pnorm(threshold, mean=PopMeans[i,j], PopSd),
                pnorm(threshold, mean=PopMeans[i,j], sqrt(PopSd^2+MeasSds[j]^2))))
  }
  print("")
}
```

```{r}
#| label: simulation
#| results: false
#| code-summary: "Code simulation"

if(F){ # redo the simulation
  # do the simulation for n samples
  set.seed(1)
  dosim(nrsimulations)
  
  # write results to file
saveRDS(list(n=n, props=props, trainprop=trainprop, testprop=testprop, MeasSds=MeasSds, 
            PopSd=PopSd, threshold=threshold, nr_meas=nr_meas, 
            tedao=tedao, trdao=trdao, dsc=dsc, dsc2=dsc2), file=paste0("SimresMatrix_",nrsimulations,"sim.rds"))
}

res<-readRDS(paste0("SimresMatrix_",nrsimulations,"sim.rds"))
str(res)
dsc<-res$dsc
dsc2<-res$dsc2
tedao<-res$tedao
trdao<-res$trdao
props<-res$props
trainprop<-res$trainprop
testprop<-res$trainprop
MeasSds<-res$MeasSds
PopSd<-res$PopSd
threshold<-res$threshold
nr_meas<-res$nr_meas
# Calculate mean, sd, UL and LL AUPR/ROC values per combination of proportion of deferrals and measurement uncertainty
lp<-length(props)
lm<-length(MeasSds)
```

```{r}
#| label: cAUPR results
#| results: false
#| code-summary: "Code performance statistics (cAUPR)"

auind<-1 # selection for AUPR (1) or ROC values (2)
aucor<-T # area under the curve is corrected (T) or not (F)
calcMedian<-F


for (l in 1:lm){
  eval(parse(text=paste0("auMean",l,"<-matrix(0, nrow=lp, ncol=lp)")))
  eval(parse(text=paste0("auSd"  ,l,"<-matrix(0, nrow=lp, ncol=lp)")))
  eval(parse(text=paste0("auUL"  ,l,"<-matrix(0, nrow=lp, ncol=lp)")))
  eval(parse(text=paste0("auLL"  ,l,"<-matrix(0, nrow=lp, ncol=lp)")))
  
  eval(parse(text=paste0("dscMean",l,"<-matrix(0, nrow=lp, ncol=lp)")))
  eval(parse(text=paste0("dscSd"  ,l,"<-matrix(0, nrow=lp, ncol=lp)")))
  eval(parse(text=paste0("dscUL"  ,l,"<-matrix(0, nrow=lp, ncol=lp)")))
  eval(parse(text=paste0("dscLL"  ,l,"<-matrix(0, nrow=lp, ncol=lp)")))
  for (i in 1:lp){
    for (j in 1:lp){
      eval(parse(text=paste0("auMean",l,"[i,j]<-mean(tedao    [,i,j,",l,",auind])")))
      if(calcMedian) eval(parse(text=paste0("auMean",l,"[i,j]<-median(tedao    [,i,j,",l,",auind])")))
      eval(parse(text=paste0("auSd"  ,l,"[i,j]<-sd(tedao      [,i,j,",l,",auind])")))
      eval(parse(text=paste0("auUL"  ,l,"[i,j]<-quantile(tedao[,i,j,",l,",auind],.975)")))
      eval(parse(text=paste0("auLL"  ,l,"[i,j]<-max(0,quantile(tedao[,i,j,",l,",auind],.025))")))
      if (auind==1 & aucor==T){ # calculated corrected AUPR values
        eval(parse(text=paste0("auMean",l,"[i,j]<-mean(tedao    [,i,j,",l,",auind]-dsc2[,i,j,",l,"])")))
        if(calcMedian) eval(parse(text=paste0("auMean",l,"[i,j]<-median(tedao    [,i,j,",l,",auind]-dsc2[,i,j,",l,"])")))
        eval(parse(text=paste0("auSd"  ,l,"[i,j]<-sd(tedao      [,i,j,",l,",auind]-dsc2[,i,j,",l,"])")))
        eval(parse(text=paste0("auUL"  ,l,"[i,j]<-quantile(tedao[,i,j,",l,",auind]-dsc2[,i,j,",l,"],.975)")))
        eval(parse(text=paste0("auLL"  ,l,"[i,j]<-max(0,quantile(tedao[,i,j,",l,",auind]-dsc2[,i,j,",l,"],.025))")))
      }
      
      eval(parse(text=paste0("dscMean",l,"[i,j]<-mean(dsc2    [,i,j,",l,"])")))
      if(calcMedian) eval(parse(text=paste0("dscMean",l,"[i,j]<-median(dsc2    [,i,j,",l,"])")))
      eval(parse(text=paste0("dscSd"  ,l,"[i,j]<-sd(dsc2      [,i,j,",l,"])")))
      eval(parse(text=paste0("dscUL"  ,l,"[i,j]<-quantile(dsc2[,i,j,",l,"],.975)")))
      eval(parse(text=paste0("dscLL"  ,l,"[i,j]<-quantile(dsc2[,i,j,",l,"],.025)")))
    }
  }
}  
auMean1
auMean2
auUL1
auLL1

auMean<-apply(trdao[,,auind],2,mean)
if(calcMedian) auMean<-apply(trdao[,,auind],2,median)
auSd  <-apply(trdao[,,auind],2,sd)
auUL  <-apply(trdao[,,auind],2,function (x) quantile(x,0.975))
auLL  <-apply(trdao[,,auind],2,function (x) quantile(x,0.025))
if (auind==1 & aucor==T){ # calculated corrected AUPR values
  auMean<-apply(trdao[,,auind]-dsc,2,mean)
  if(calcMedian) auMean<-apply(trdao[,,auind]-dsc,2,median)
  auSd  <-apply(trdao[,,auind]-dsc,2,sd)
  auUL  <-apply(trdao[,,auind]-dsc,2,function (x) quantile(x,0.975))
  auLL  <-apply(trdao[,,auind]-dsc,2,function (x) quantile(x,0.025))
}   
(dscMean<-apply(dsc,2,mean))
if(calcMedian) (dscMean<-apply(dsc,2,median))
dscUL   <-apply(dsc,2,function (x) quantile(x,0.975))
dscLL   <-apply(dsc,2,function (x) quantile(x,0.025))
auMean
auUL
auLL


```

```{r}
#| label: fig-defrates
#| code-summary: "Code plot"
#| fig-cap: The proportion of donors below the deferral threshold for the training and test data sets in the simulation as function of the preset proportion of deferral (1%, 2%, 4%, 8% and 16%).
#| fig-cap-location: margin
d<-min(props)/12
plot(c(props-d,props+d),c(dscMean,diag(dscMean1)), pch=rep(c(1,4),each=lp), ylim=c(0,.2), 
     ylab="Proportion deferrals in sample", xlab="Proportion deferrals")
for (i in 1:lp) {
  lines(rep(props[i]-d,2),c(dscLL [i],   dscUL [i]))
  lines(rep(props[i]+d,2),c(dscLL1[i,i], dscUL1[i,i]))
}
legend("bottomright", c("Training data", "Test data", "95% Confidence interval"), lty=c(NA,NA,1), pch=c(1,4,NA))

```

## cAUPR

```{r}
#| label: fig-AUPR
#| fig-cap: Mean area under the corrected precision-recall curve (cAUPR) over all simulations and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals.
#| fig-cap-location: margin
#| code-summary: "Code plot"

# plot AUPR/ROC results
if(auind==1){
  if (aucor==T){ # corrected AUPR values
    plot(c(props-d,props+d),c(auMean,diag(auMean1)), pch=rep(c(1,4),each=lp), ylim=c(0,.3),
         ylab="Corrected area under the PR curve", xlab="Proportion deferrals")
  } else { # corrected AUPR values
    plot(c(props-d,props+d),c(auMean,diag(auMean1)), pch=rep(c(1,4),each=lp), ylim=c(0,.5),
       ylab="Area under the PR curve", xlab="Proportion deferrals")
  }
} else {
    plot(c(props-d,props+d),c(auMean,diag(auMean1)), pch=rep(c(1,4),each=lp), ylim=c(0,1),
         ylab="Area under the ROC curve", xlab="Proportion deferrals")
}
for (i in 1:lp) {
  lines(rep(props[i]-d,2),c(auLL[i], auUL[i]))
  lines(rep(props[i]+d,2),c(auLL1[i,i], auUL1[i,i]))
}
legend("bottomright", c("Training data", "Test data", "95% Confidence interval"), lty=c(NA,NA,1), pch=c(1,4,NA))
if(auind==2) abline(h=max(auMean), col=8, lty=2)
```

```{r}
#| label: fig-AUPRexchange
#| fig-cap: Mean area under the corrected precision-recall curve (cAUPR) and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals. Models developed on training data at each of the deferral proportion levels are applied on all test datasets.
#| fig-cap-location: margin
#| code-summary: "Code plot"

# create plots
# plot main training results
cols<-c(1,2,3,4,6)
plotboth <- F
if(auind==1) { # AUPR plots
  if (aucor==T){ # corrected AUPR values
    ylab="Corrected area under the PR curve"
    if(plotboth){
      ylim<-c(0,0.45)
    } else { # if plotted with alternative deferral probability dataset
      ylim<-c(0,0.3)
    }
  } else {
    ylab="Area under the PR curve"
    if(plotboth){
      ylim<-c(0,0.65)
    } else { # if plotted with alternative deferral probability dataset
      ylim<-c(0,0.5)
    }
  }
} else { # AUROC plots
  ylab="Area under the AUROC curve"
  if(plotboth){
    ylim<-c(.5,1)
  } else { # if plotted with alternative deferral probability dataset
    ylim<-c(.5,.9)
  }
}
plot(props-d,auMean, pch=rep(1,lp), ylim=ylim, xlim=c(0,0.17), col=cols,
     ylab=ylab, xlab="Proportion deferrals")
# add training uncertainty
for (i in 1:lp) lines(rep(props[i]-d,2),c(auLL[i], auUL[i]), col=cols[i])
# add validation results and uncertainty estimates
for (i in 1:lp) {
  lines(rep(props[i]+2*d,2), c(auLL1[1,i], auUL1[1,i]), col=cols[1])
  lines(rep(props[i]+3*d,2), c(auLL1[2,i], auUL1[2,i]), col=cols[2])
  lines(rep(props[i]+4*d,2), c(auLL1[3,i], auUL1[3,i]), col=cols[3])
  lines(rep(props[i]+5*d,2), c(auLL1[4,i], auUL1[4,i]), col=cols[4])
  lines(rep(props[i]+6*d,2), c(auLL1[5,i], auUL1[5,i]), col=cols[5])
  
  points(props[i]+2*d, auMean1[1,i], pch=4, col=cols[1])
  points(props[i]+3*d, auMean1[2,i], pch=4, col=cols[2])
  points(props[i]+4*d, auMean1[3,i], pch=4, col=cols[3])
  points(props[i]+5*d, auMean1[4,i], pch=4, col=cols[4])
  points(props[i]+6*d, auMean1[5,i], pch=4, col=cols[5])
  
  if (F) {
    points(props[i]+2*d, auUL1[1,i], pch=1)
    points(props[i]+3*d, auUL1[2,i], pch=1)
    points(props[i]+4*d, auUL1[3,i], pch=1)
    points(props[i]+5*d, auUL1[4,i], pch=1)
    points(props[i]+6*d, auUL1[5,i], pch=1)
    
    points(props[i]+2*d, auLL1[1,i], pch=1)
    points(props[i]+3*d, auLL1[2,i], pch=1)
    points(props[i]+4*d, auLL1[3,i], pch=1)
    points(props[i]+5*d, auLL1[4,i], pch=1)
    points(props[i]+6*d, auLL1[5,i], pch=1)
  }
}

# add legends
legend(0.11,ylim[1]+.85*(ylim[2]-ylim[1]), paste0(props*100,"%"), title="proportion \ndeferrals\nin dataset",
       col=cols, lty=rep(1,lp), box.col=0, cex=0.75)
if(!plotboth){
  legend(0.095, ylim[1]+.3*(ylim[2]-ylim[1]), c("Training data", "Test data (increasing deferral rate", 
          "training data from left to right)", "95% Confidence interval"), 
           lty=c(NA,NA,NA,1), pch=c(1,4,NA,NA), cex=0.75)
}
```

```{r}
#| label: tbl-cAUPR
#| code-summary: "Code table"
#| tbl-cap: "AUPR estimates and SD of Hb deferral prediction models developed on and applied to datasets with varying deferral rates and measurement errors."
#| tbl-cap-location: margin

aupr05 <- matrix(nrow=nrow(auMean1), ncol = ncol(auMean1)+ncol(auSd1))
for(i in 1:5){
  aupr05[1:5,(2*i)-1] <- auMean1[1:5,i] 
  aupr05[1:5,(2*i)] <- auSd1[1:5,i] 
}

aupr025 <- matrix(nrow=nrow(auMean2), ncol = ncol(auMean2)+ncol(auSd2))
for(i in 1:5){
  aupr025[,(2*i)-1] <- auMean2[,i] 
  aupr025[,(2*i)] <- auSd2[,i] 
}

rownames<- c("Training 1%", "Training 2%", "Training 4%", "Training 8%", "Training 16%")
colnames<- c("cAUPR", "Sd", "cAUPR", "Sd","cAUPR", "Sd","cAUPR", "Sd","cAUPR", "Sd")

auprtable <- rbind(aupr05, aupr025)
rownames(auprtable)<-c(rownames, rownames)
colnames(auprtable)<-colnames
kable(auprtable,
      digits=3,
      row.names=T
      )%>% add_header_above(c(" " = 1,"Test 1%"=2, "Test 2%"=2, "Test 4%"=2, "Test 8%"=2, "Test 16%"=2)) %>% pack_rows(index = c("Measurement error 0.5 mmol/L" = 5, "Measurement error 0.25 mmol/L" = 5))
```


```{r}
#| label: fig-AUPRbothmeaserr
#| fig-cap: Mean area under the corrected precision-recall curve (cAUPR) and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals. Models developed on training data at each of the deferral proportion levels are applied on all test datasets. In addition, cAUPR estimates for the prediction models are shown when applied to test datasets with lower measurement error (0.25 mmol/L).
#| fig-cap-location: margin
#| code-summary: "Code plot"

plotboth <- T
auind <- 1
if(auind==1) { # AUPR plots
  if (aucor==T){ # corrected AUPR values
    ylab="Corrected area under the PR curve"
    if(plotboth){
      ylim<-c(0,0.45)
    } else { # if plotted with alternative deferral probability dataset
      ylim<-c(0,0.3)
    }
  } else {
    ylab="Area under the PR curve"
    if(plotboth){
      ylim<-c(0,0.65)
    } else { # if plotted with alternative deferral probability dataset
      ylim<-c(0,0.5)
    }
  }
} else { # AUROC plots
  ylab="Area under the AUROC curve"
  if(plotboth){
    ylim<-c(.5,1)
  } else { # if plotted with alternative deferral probability dataset
    ylim<-c(.5,.9)
  }
}
plot(props-d,auMean, pch=rep(1,lp), ylim=ylim, xlim=c(0,0.17), col=cols,
     ylab=ylab, xlab="Proportion deferrals")
# add training uncertainty
for (i in 1:lp) lines(rep(props[i]-d,2),c(auLL[i], auUL[i]), col=cols[i])
# add validation results and uncertainty estimates
for (i in 1:lp) {
  lines(rep(props[i]+2*d,2), c(auLL1[1,i], auUL1[1,i]), col=cols[1])
  lines(rep(props[i]+3*d,2), c(auLL1[2,i], auUL1[2,i]), col=cols[2])
  lines(rep(props[i]+4*d,2), c(auLL1[3,i], auUL1[3,i]), col=cols[3])
  lines(rep(props[i]+5*d,2), c(auLL1[4,i], auUL1[4,i]), col=cols[4])
  lines(rep(props[i]+6*d,2), c(auLL1[5,i], auUL1[5,i]), col=cols[5])
  
  points(props[i]+2*d, auMean1[1,i], pch=4, col=cols[1])
  points(props[i]+3*d, auMean1[2,i], pch=4, col=cols[2])
  points(props[i]+4*d, auMean1[3,i], pch=4, col=cols[3])
  points(props[i]+5*d, auMean1[4,i], pch=4, col=cols[4])
  points(props[i]+6*d, auMean1[5,i], pch=4, col=cols[5])
  
  if (F) {
    points(props[i]+2*d, auUL1[1,i], pch=1)
    points(props[i]+3*d, auUL1[2,i], pch=1)
    points(props[i]+4*d, auUL1[3,i], pch=1)
    points(props[i]+5*d, auUL1[4,i], pch=1)
    points(props[i]+6*d, auUL1[5,i], pch=1)
    
    points(props[i]+2*d, auLL1[1,i], pch=1)
    points(props[i]+3*d, auLL1[2,i], pch=1)
    points(props[i]+4*d, auLL1[3,i], pch=1)
    points(props[i]+5*d, auLL1[4,i], pch=1)
    points(props[i]+6*d, auLL1[5,i], pch=1)
  }
}

# add legends
legend(0.11,ylim[1]+.85*(ylim[2]-ylim[1]), paste0(props*100,"%"), title="proportion \ndeferrals\nin dataset",
       col=cols, lty=rep(1,lp), box.col=0, cex=0.65)
if(!plotboth){
  legend(0.095, ylim[1]+.3*(ylim[2]-ylim[1]), c("Training data", "Test data (increasing deferral rate", 
          "training data from left to right)", "95% Confidence interval"), 
           lty=c(NA,NA,NA,1), pch=c(1,4,NA,NA),cex=0.5)
}

# plot validation plots for dataset with another measurement variability if required
if (plotboth) { 
  d2=d/2
  auMean<-apply(trdao[,,2],2,mean)
  if(calcMedian) auMean<-apply(trdao[,,2],2,median)
  auSd  <-apply(trdao[,,2],2,sd)
  auLL  <-apply(trdao[,,2],2,function (x) quantile(x,0.975))
  auUL  <-apply(trdao[,,2],2,function (x) quantile(x,0.025))
  for (i in 1:lp) {
    lines(rep(props[i]+2*d+d2,2), c(auLL2[1,i], auUL2[1,i]), col=cols[1])
    lines(rep(props[i]+3*d+d2,2), c(auLL2[2,i], auUL2[2,i]), col=cols[2])
    lines(rep(props[i]+4*d+d2,2), c(auLL2[3,i], auUL2[3,i]), col=cols[3])
    lines(rep(props[i]+5*d+d2,2), c(auLL2[4,i], auUL2[4,i]), col=cols[4])
    lines(rep(props[i]+6*d+d2,2), c(auLL2[5,i], auUL2[5,i]), col=cols[5])
    
    points(props[i]+2*d+d2, auMean2[1,i], pch=2, col=cols[1])
    points(props[i]+3*d+d2, auMean2[2,i], pch=2, col=cols[2])
    points(props[i]+4*d+d2, auMean2[3,i], pch=2, col=cols[3])
    points(props[i]+5*d+d2, auMean2[4,i], pch=2, col=cols[4])
    points(props[i]+6*d+d2, auMean2[5,i], pch=2, col=cols[5])
  }
  legend(0.095, ylim[1]+.4*(ylim[2]-ylim[1]), c("Training data", "Test data 1 (increasing deferral rates", 
          "in training data from left to right)", paste0("Sd of Measurements=",
          round(MeasSds[1],2)),  "Test data 2 ", paste0("Sd of Measurements=",
          round(MeasSds[2],2)),"95% Confidence interval"), cex=0.5,
          lty=c(NA,NA,NA,NA,NA,NA,1), pch=c(1,4,NA,NA,2,NA,NA))
}
if(auind==2) {
  abline(h=max(auMean), col=8, lty=2)
  legend(0.11,ylim[1]+.85*(ylim[2]-ylim[1]), paste0(props*100,"%"), title="proportion \ndeferrals\nin dataset",
       col=cols, lty=rep(1,lp), box.col=0, cex=0.75)
}
```

## AUROC
```{r}
#| label: AUROC results
#| results: false
#| code-summary: "Code performance statistics (AUROC)"

auind<-2 # selection for AUPR (1) or ROC values (2)
aucor<-F # area under the curve is corrected (T) or not (F)
aucor<-T # area under the curve is corrected (T) or not (F)
calcMedian<-F


for (l in 1:lm){
  eval(parse(text=paste0("auMean",l,"<-matrix(0, nrow=lp, ncol=lp)")))
  eval(parse(text=paste0("auSd"  ,l,"<-matrix(0, nrow=lp, ncol=lp)")))
  eval(parse(text=paste0("auUL"  ,l,"<-matrix(0, nrow=lp, ncol=lp)")))
  eval(parse(text=paste0("auLL"  ,l,"<-matrix(0, nrow=lp, ncol=lp)")))
  
  eval(parse(text=paste0("dscMean",l,"<-matrix(0, nrow=lp, ncol=lp)")))
  eval(parse(text=paste0("dscSd"  ,l,"<-matrix(0, nrow=lp, ncol=lp)")))
  eval(parse(text=paste0("dscUL"  ,l,"<-matrix(0, nrow=lp, ncol=lp)")))
  eval(parse(text=paste0("dscLL"  ,l,"<-matrix(0, nrow=lp, ncol=lp)")))
  for (i in 1:lp){
    for (j in 1:lp){
      eval(parse(text=paste0("auMean",l,"[i,j]<-mean(tedao    [,i,j,",l,",auind])")))
      if(calcMedian) eval(parse(text=paste0("auMean",l,"[i,j]<-median(tedao    [,i,j,",l,",auind])")))
      eval(parse(text=paste0("auSd"  ,l,"[i,j]<-sd(tedao      [,i,j,",l,",auind])")))
      eval(parse(text=paste0("auUL"  ,l,"[i,j]<-quantile(tedao[,i,j,",l,",auind],.975)")))
      eval(parse(text=paste0("auLL"  ,l,"[i,j]<-max(0,quantile(tedao[,i,j,",l,",auind],.025))")))
      if (auind==1 & aucor==T){ # calculated corrected AUPR values
        eval(parse(text=paste0("auMean",l,"[i,j]<-mean(tedao    [,i,j,",l,",auind]-dsc2[,i,j,",l,"])")))
        if(calcMedian) eval(parse(text=paste0("auMean",l,"[i,j]<-median(tedao    [,i,j,",l,",auind]-dsc2[,i,j,",l,"])")))
        eval(parse(text=paste0("auSd"  ,l,"[i,j]<-sd(tedao      [,i,j,",l,",auind]-dsc2[,i,j,",l,"])")))
        eval(parse(text=paste0("auUL"  ,l,"[i,j]<-quantile(tedao[,i,j,",l,",auind]-dsc2[,i,j,",l,"],.975)")))
        eval(parse(text=paste0("auLL"  ,l,"[i,j]<-max(0,quantile(tedao[,i,j,",l,",auind]-dsc2[,i,j,",l,"],.025))")))
      }
      
      eval(parse(text=paste0("dscMean",l,"[i,j]<-mean(dsc2    [,i,j,",l,"])")))
      if(calcMedian) eval(parse(text=paste0("dscMean",l,"[i,j]<-median(dsc2    [,i,j,",l,"])")))
      eval(parse(text=paste0("dscSd"  ,l,"[i,j]<-sd(dsc2      [,i,j,",l,"])")))
      eval(parse(text=paste0("dscUL"  ,l,"[i,j]<-quantile(dsc2[,i,j,",l,"],.975)")))
      eval(parse(text=paste0("dscLL"  ,l,"[i,j]<-quantile(dsc2[,i,j,",l,"],.025)")))
    }
  }
}  
auMean1
auMean2
auUL1
auLL1

auMean<-apply(trdao[,,auind],2,mean)
if(calcMedian) auMean<-apply(trdao[,,auind],2,median)
auSd  <-apply(trdao[,,auind],2,sd)
auUL  <-apply(trdao[,,auind],2,function (x) quantile(x,0.975))
auLL  <-apply(trdao[,,auind],2,function (x) quantile(x,0.025))
if (auind==1 & aucor==T){ # calculated corrected AUPR values
  auMean<-apply(trdao[,,auind]-dsc,2,mean)
  if(calcMedian) auMean<-apply(trdao[,,auind]-dsc,2,median)
  auSd  <-apply(trdao[,,auind]-dsc,2,sd)
  auUL  <-apply(trdao[,,auind]-dsc,2,function (x) quantile(x,0.975))
  auLL  <-apply(trdao[,,auind]-dsc,2,function (x) quantile(x,0.025))
}   
(dscMean<-apply(dsc,2,mean))
if(calcMedian) (dscMean<-apply(dsc,2,median))
dscUL   <-apply(dsc,2,function (x) quantile(x,0.975))
dscLL   <-apply(dsc,2,function (x) quantile(x,0.025))
auMean
auUL
auLL

auroc05 <- matrix(nrow=nrow(auMean1), ncol = ncol(auMean1)+ncol(auSd1))
for(i in 1:5){
  auroc05[1:5,(2*i)-1] <- auMean1[1:5,i] 
  auroc05[1:5,(2*i)] <- auSd1[1:5,i] 
}
auroc025 <- matrix(nrow=nrow(auMean2), ncol = ncol(auMean2)+ncol(auSd2))
for(i in 1:5){
  auroc025[,(2*i)-1] <- auMean2[,i] 
  auroc025[,(2*i)] <- auSd2[,i] 
}
```

```{r}
#| label: fig-AUROC
#| fig-cap: Mean area under the ROC curve (AUROC) over all simulations and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals. For ease of comparison a grey dashed line is added that is equal to the highest mean AUROC value over all training sets.
#| code-summary: "Code plot"
#| fig-cap-location: margin

# plot AUPR/ROC results
if(auind==1){
  if (aucor==T){ # corrected AUPR values
    plot(c(props-d,props+d),c(auMean,diag(auMean1)), pch=rep(c(1,4),each=lp), ylim=c(0,.3),
         ylab="Corrected area under the PR curve", xlab="Proportion deferrals")
  } else { # corrected AUPR values
    plot(c(props-d,props+d),c(auMean,diag(auMean1)), pch=rep(c(1,4),each=lp), ylim=c(0,.5),
       ylab="Area under the PR curve", xlab="Proportion deferrals")
  }
} else {
    plot(c(props-d,props+d),c(auMean,diag(auMean1)), pch=rep(c(1,4),each=lp), ylim=c(0,1),
         ylab="Area under the ROC curve", xlab="Proportion deferrals")
}
for (i in 1:lp) {
  lines(rep(props[i]-d,2),c(auLL[i], auUL[i]))
  lines(rep(props[i]+d,2),c(auLL1[i,i], auUL1[i,i]))
}
legend("bottomright", c("Training data", "Test data", "95% Confidence interval"), lty=c(NA,NA,1), pch=c(1,4,NA))
if(auind==2) abline(h=max(auMean), col=8, lty=2)
```

```{r}
#| label: fig-AUROCexchange
#| fig-cap: Mean area under the ROC curve (AUROC) and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals. Models developed on training data at each of the deferral proportion levels are applied on all test datasets. For ease of comparison a grey dashed line is added that is equal to the highest mean AUROC value over all training sets.
#| code-summary: "Code plot"
#| fig-cap-location: margin

# create plots
# plot main training results
cols<-c(1,2,3,4,6)
plotboth <- F
if(auind==1) { # AUPR plots
  if (aucor==T){ # corrected AUPR values
    ylab="Corrected area under the PR curve"
    if(plotboth){
      ylim<-c(0,0.45)
    } else { # if plotted with alternative deferral probability dataset
      ylim<-c(0,0.3)
    }
  } else {
    ylab="Area under the PR curve"
    if(plotboth){
      ylim<-c(0,0.65)
    } else { # if plotted with alternative deferral probability dataset
      ylim<-c(0,0.5)
    }
  }
} else { # AUROC plots
  ylab="Area under the AUROC curve"
  if(plotboth){
    ylim<-c(.5,1)
  } else { # if plotted with alternative deferral probability dataset
    ylim<-c(.5,.9)
  }
}
plot(props-d,auMean, pch=rep(1,lp), ylim=ylim, xlim=c(0,0.17), col=cols,
     ylab=ylab, xlab="Proportion deferrals")
# add training uncertainty
for (i in 1:lp) lines(rep(props[i]-d,2),c(auLL[i], auUL[i]), col=cols[i])
# add validation results and uncertainty estimates
for (i in 1:lp) {
  lines(rep(props[i]+2*d,2), c(auLL1[1,i], auUL1[1,i]), col=cols[1])
  lines(rep(props[i]+3*d,2), c(auLL1[2,i], auUL1[2,i]), col=cols[2])
  lines(rep(props[i]+4*d,2), c(auLL1[3,i], auUL1[3,i]), col=cols[3])
  lines(rep(props[i]+5*d,2), c(auLL1[4,i], auUL1[4,i]), col=cols[4])
  lines(rep(props[i]+6*d,2), c(auLL1[5,i], auUL1[5,i]), col=cols[5])
  
  points(props[i]+2*d, auMean1[1,i], pch=4, col=cols[1])
  points(props[i]+3*d, auMean1[2,i], pch=4, col=cols[2])
  points(props[i]+4*d, auMean1[3,i], pch=4, col=cols[3])
  points(props[i]+5*d, auMean1[4,i], pch=4, col=cols[4])
  points(props[i]+6*d, auMean1[5,i], pch=4, col=cols[5])
  
  if (F) {
    points(props[i]+2*d, auUL1[1,i], pch=1)
    points(props[i]+3*d, auUL1[2,i], pch=1)
    points(props[i]+4*d, auUL1[3,i], pch=1)
    points(props[i]+5*d, auUL1[4,i], pch=1)
    points(props[i]+6*d, auUL1[5,i], pch=1)
    
    points(props[i]+2*d, auLL1[1,i], pch=1)
    points(props[i]+3*d, auLL1[2,i], pch=1)
    points(props[i]+4*d, auLL1[3,i], pch=1)
    points(props[i]+5*d, auLL1[4,i], pch=1)
    points(props[i]+6*d, auLL1[5,i], pch=1)
  }
}

# add legends
legend(0.11,ylim[1]+.85*(ylim[2]-ylim[1]), paste0(props*100,"%"), title="proportion \ndeferrals\nin dataset",
       col=cols, lty=rep(1,lp), box.col=0, cex=0.75)
if(!plotboth){
  legend(0.095, ylim[1]+.3*(ylim[2]-ylim[1]), c("Training data", "Test data (increasing deferral rate", 
          "training data from left to right)", "95% Confidence interval"), 
           lty=c(NA,NA,NA,1), pch=c(1,4,NA,NA), cex=0.75)
}
```

```{r}
#| label: tbl-AUROC
#| code-summary: "Code table"
#| tbl-cap: "AUROC estimates and SD of Hb deferral prediction models developed on and applied to datasets with varying deferral rates and measurement errors."
#| tbl-cap-location: margin
rownames<- c("Training 1%", "Training 2%", "Training 4%", "Training 8%", "Training 16%")
colnames<- c("cAUPR", "Sd", "cAUPR", "Sd","cAUPR", "Sd","cAUPR", "Sd","cAUPR", "Sd")

auroctable <- rbind(auroc05, auroc025)
rownames(auroctable)<-c(rownames, rownames)
colnames(auroctable)<-colnames
kable(auroctable,
      digits=3,
      row.names=T
)%>% add_header_above(c(" " = 1,"Test 1%"=2, "Test 2%"=2, "Test 4%"=2, "Test 8%"=2, "Test 16%"=2)) %>% pack_rows(index = c("Measurement error 0.5 mmol/L" = 5, "Measurement error 0.25 mmol/L" = 5))
```


```{r}
#| label: fig-AUROCbothmeaserr
#| fig-cap: Mean area under the ROC curve (AUROC) and the spread in the estimates for the individual training and test data sets as a function of the proportion of deferrals. Models developed on training data at each of the deferral proportion levels are applied on all test datasets. In addition, AUROC estimates for the prediction models are shown when applied to test datasets with lower measurement error (0.25 mmol/L). For ease of comparison a grey dashed line is added that is equal to the highest mean AUROC value over all training sets.
#| fig-cap-location: margin
#| code-summary: "Code plot"
plotboth <- T
auind <- 2
if(auind==1) { # AUPR plots
  if (aucor==T){ # corrected AUPR values
    ylab="Corrected area under the PR curve"
    if(plotboth){
      ylim<-c(0,0.45)
    } else { # if plotted with alternative deferral probability dataset
      ylim<-c(0,0.3)
    }
  } else {
    ylab="Area under the PR curve"
    if(plotboth){
      ylim<-c(0,0.65)
    } else { # if plotted with alternative deferral probability dataset
      ylim<-c(0,0.5)
    }
  }
} else { # AUROC plots
  ylab="Area under the AUROC curve"
  if(plotboth){
    ylim<-c(.5,1)
  } else { # if plotted with alternative deferral probability dataset
    ylim<-c(.5,.9)
  }
}
plot(props-d,auMean, pch=rep(1,lp), ylim=ylim, xlim=c(0,0.17), col=cols,
     ylab=ylab, xlab="Proportion deferrals")
# add training uncertainty
for (i in 1:lp) lines(rep(props[i]-d,2),c(auLL[i], auUL[i]), col=cols[i])
# add validation results and uncertainty estimates
for (i in 1:lp) {
  lines(rep(props[i]+2*d,2), c(auLL1[1,i], auUL1[1,i]), col=cols[1])
  lines(rep(props[i]+3*d,2), c(auLL1[2,i], auUL1[2,i]), col=cols[2])
  lines(rep(props[i]+4*d,2), c(auLL1[3,i], auUL1[3,i]), col=cols[3])
  lines(rep(props[i]+5*d,2), c(auLL1[4,i], auUL1[4,i]), col=cols[4])
  lines(rep(props[i]+6*d,2), c(auLL1[5,i], auUL1[5,i]), col=cols[5])
  
  points(props[i]+2*d, auMean1[1,i], pch=4, col=cols[1])
  points(props[i]+3*d, auMean1[2,i], pch=4, col=cols[2])
  points(props[i]+4*d, auMean1[3,i], pch=4, col=cols[3])
  points(props[i]+5*d, auMean1[4,i], pch=4, col=cols[4])
  points(props[i]+6*d, auMean1[5,i], pch=4, col=cols[5])
  
  if (F) {
    points(props[i]+2*d, auUL1[1,i], pch=1)
    points(props[i]+3*d, auUL1[2,i], pch=1)
    points(props[i]+4*d, auUL1[3,i], pch=1)
    points(props[i]+5*d, auUL1[4,i], pch=1)
    points(props[i]+6*d, auUL1[5,i], pch=1)
    
    points(props[i]+2*d, auLL1[1,i], pch=1)
    points(props[i]+3*d, auLL1[2,i], pch=1)
    points(props[i]+4*d, auLL1[3,i], pch=1)
    points(props[i]+5*d, auLL1[4,i], pch=1)
    points(props[i]+6*d, auLL1[5,i], pch=1)
  }
}

# add legends
legend(0.11,ylim[1]+.85*(ylim[2]-ylim[1]), paste0(props*100,"%"), title="proportion \ndeferrals\nin dataset",
       col=cols, lty=rep(1,lp), box.col=0, cex=0.65)
if(!plotboth){
  legend(0.095, ylim[1]+.3*(ylim[2]-ylim[1]), c("Training data", "Test data (increasing deferral rate", 
          "training data from left to right)", "95% Confidence interval"), 
           lty=c(NA,NA,NA,1), pch=c(1,4,NA,NA), cex=0.5)
}

# plot validation plots for dataset with another measurement variability if required
if (plotboth) { 
  d2=d/2
  auMean<-apply(trdao[,,2],2,mean)
  if(calcMedian) auMean<-apply(trdao[,,2],2,median)
  auSd  <-apply(trdao[,,2],2,sd)
  auLL  <-apply(trdao[,,2],2,function (x) quantile(x,0.975))
  auUL  <-apply(trdao[,,2],2,function (x) quantile(x,0.025))
  for (i in 1:lp) {
    lines(rep(props[i]+2*d+d2,2), c(auLL2[1,i], auUL2[1,i]), col=cols[1])
    lines(rep(props[i]+3*d+d2,2), c(auLL2[2,i], auUL2[2,i]), col=cols[2])
    lines(rep(props[i]+4*d+d2,2), c(auLL2[3,i], auUL2[3,i]), col=cols[3])
    lines(rep(props[i]+5*d+d2,2), c(auLL2[4,i], auUL2[4,i]), col=cols[4])
    lines(rep(props[i]+6*d+d2,2), c(auLL2[5,i], auUL2[5,i]), col=cols[5])
    
    points(props[i]+2*d+d2, auMean2[1,i], pch=2, col=cols[1])
    points(props[i]+3*d+d2, auMean2[2,i], pch=2, col=cols[2])
    points(props[i]+4*d+d2, auMean2[3,i], pch=2, col=cols[3])
    points(props[i]+5*d+d2, auMean2[4,i], pch=2, col=cols[4])
    points(props[i]+6*d+d2, auMean2[5,i], pch=2, col=cols[5])
  }
  legend(0.095, ylim[1]+.4*(ylim[2]-ylim[1]), c("Training data", "Test data 1 (increasing deferral rates", 
          "in training data from left to right)", paste0("Sd of Measurements=",
          round(MeasSds[1],2)),  "Test data 2 ", paste0("Sd of Measurements=",
          round(MeasSds[2],2)),"95% Confidence interval"), cex=0.5,
          lty=c(NA,NA,NA,NA,NA,NA,1), pch=c(1,4,NA,NA,2,NA,NA))
}

```
